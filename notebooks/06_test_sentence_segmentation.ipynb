{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento - Segmentación de Oraciones con NLTK\n",
    "\n",
    "**Objetivo:** Dividir los documentos en oraciones y luego aplicar el preprocesamiento (tokenización, POS, lematización) a cada oración. Usaremos `nltk.send_tokenize` y adaptaremos nuestro pipeline para procesar oracion por oracion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    print(f\"Añadido '{module_path}' a sys.path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functions import batch_generator, preprocess_text_step1, get_wordnet_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "global lematizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_data = fetch_20newsgroups(subset='all',\n",
    "                                     remove=('headers', 'footers', 'quotes'),\n",
    "                                     data_home='./data/20newsgroups_cache')\n",
    "documents = newsgroups_data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_sentence_nltk(sentence_text):\n",
    "    \"\"\"Tokeniza, etiqueta POS y lematiza una única oración (string).\"\"\"\n",
    "    tokens = nltk.word_tokenize(sentence_text, language='english')\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    lemmas = []\n",
    "    for word, tag in pos_tags:\n",
    "        if word.isalnum():\n",
    "            wordnet_tag = get_wordnet_pos(tag)\n",
    "            lemma = lemmatizer.lemmatize(word, pos=wordnet_tag)\n",
    "            lemmas.append(lemma)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función principal que orquesta el pipeline para 1 documento\n",
    "def segment_and_process_document(doc_text):\n",
    "    \"\"\"\n",
    "    Normaliza, segmenta en oraciones y lematiza cada oración de un documento.\n",
    "\n",
    "    Args:\n",
    "        doc_text (str): Texto crudo del documento.\n",
    "\n",
    "    Returns:\n",
    "        list[list[str]]: Lista de oraciones, donde cada oración es una lista de lemas.\n",
    "    \"\"\"\n",
    "    # normalizamos los documentos previamente a la segmentación\n",
    "    normalized_doc = preprocess_text_step1(doc_text)\n",
    "\n",
    "    # usamos el segmentor de oraciones de nltk para dividir el texto en oraciones\n",
    "    sentences = nltk.sent_tokenize(normalized_doc, language='english')\n",
    "\n",
    "    # lematizamos cada oración y la agregamos a una lista\n",
    "    processed_doc = []\n",
    "    for sentence in sentences:\n",
    "        lemmatized_sentence = lemmatize_sentence_nltk(sentence)\n",
    "        # nos aseguramos de no agregar  listas vacías si una oración solo contenía puntuación\n",
    "        if lemmatized_sentence:\n",
    "            processed_doc.append(lemmatized_sentence)\n",
    "\n",
    "    return processed_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***recordar**: Los tokens se pueden ver como una palabra en una oracion o una oracion en un parrafo*\n",
    "\n",
    "`nltk.sent_tokenize` lo que hace es tokenizar el texto en oraciones, a diferencia de `nltk.word_tokenize` que tokeniza por palabras en una oracion.\n",
    "\n",
    "Por que hacemos esto? Porque muchos modelos de lenguajes procesan la informacion oracion por oracion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Obteniendo el primer lote (batch_size = 10)...\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "print(f\"\\nObteniendo el primer lote (batch_size = {batch_size})...\")\n",
    "batch_gen = batch_generator(documents, batch_size)\n",
    "first_batch = next(batch_gen, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lote procesado en 0.0261 segundos.\n"
     ]
    }
   ],
   "source": [
    "start_proc_time = time.time()\n",
    "# processed_batch ahora contendrá una lista de listas de listas\n",
    "# [doc1, doc2, ...] donde doc1 = [sent1_lemmas, sent2_lemmas, ...] y sent1_lemmas = [lemma1, lemma2, ...]\n",
    "processed_batch_segmented = [segment_and_process_document(doc) for doc in first_batch]\n",
    "end_proc_time = time.time()\n",
    "print(f\"Lote procesado en {end_proc_time - start_proc_time:.4f} segundos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mostramos los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mostrando los primeros 6 ejemplos procesados:\n",
      "\n",
      "--- Documento 1 ---\n",
      "  Original (712 chars):\n",
      "'\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "'\n",
      "\n",
      "  Número de oraciones detectadas: 9\n",
      "\n",
      "  Primeras oraciones procesadas (lemas):\n",
      "    Oración 1 (27 lemas): ['i', 'be', 'sure', 'some', 'bashers', 'of', 'pen', 'fan', 'be', 'pretty', 'confuse', 'about', 'the', 'lack', 'of', 'any', 'kind', 'of', 'post', 'about', 'the', 'recent', 'pen', 'massacre', 'of', 'the', 'devil']\n",
      "    Oración 2 (10 lemas): ['actually', 'i', 'be', 'bit', 'puzzled', 'too', 'and', 'a', 'bit', 'relieved']\n",
      "    Oración 3 (18 lemas): ['however', 'i', 'be', 'go', 'to', 'put', 'an', 'end', 'to', 'relief', 'with', 'a', 'bit', 'of', 'praise', 'for', 'the', 'pen']\n",
      "    Oración 4 (10 lemas): ['man', 'they', 'be', 'kill', 'those', 'devil', 'bad', 'than', 'i', 'thought']\n",
      "    Oración 5 (14 lemas): ['jagr', 'just', 'show', 'you', 'why', 'he', 'be', 'much', 'good', 'than', 'his', 'regular', 'season', 'stats']\n",
      "    Oración 6 (12 lemas): ['he', 'be', 'also', 'a', 'lot', 'fo', 'fun', 'to', 'watch', 'in', 'the', 'playoff']\n",
      "    Oración 7 (28 lemas): ['bowman', 'should', 'let', 'jagr', 'have', 'a', 'lot', 'of', 'fun', 'in', 'the', 'next', 'couple', 'of', 'game', 'since', 'the', 'pen', 'be', 'go', 'to', 'beat', 'the', 'pulp', 'out', 'of', 'jersey', 'anyway']\n",
      "    Oración 8 (15 lemas): ['i', 'be', 'very', 'disappointed', 'not', 'to', 'see', 'the', 'islander', 'lose', 'the', 'final', 'regular', 'season', 'game']\n",
      "    Oración 9 (2 lemas): ['pen', 'rule']\n",
      "\n",
      "--- Documento 2 ---\n",
      "  Original (324 chars):\n",
      "'My brother is in the market for a high-performance video card that supports\n",
      "VESA local bus with 1-2MB RAM.  Does anyone have suggestions/ideas on:\n",
      "\n",
      "  - Diamond Stealth Pro Local Bus\n",
      "\n",
      "  - Orchid Farenheit 1280\n",
      "\n",
      "  - ATI Graphics Ultra Pro\n",
      "\n",
      "  - Any other high-performance VLB card\n",
      "\n",
      "\n",
      "Please post or email.  Thank you!\n",
      "\n",
      "  - Matt\n",
      "'\n",
      "\n",
      "  Número de oraciones detectadas: 4\n",
      "\n",
      "  Primeras oraciones procesadas (lemas):\n",
      "    Oración 1 (17 lemas): ['my', 'brother', 'be', 'in', 'the', 'market', 'for', 'a', 'video', 'card', 'that', 'support', 'vesa', 'local', 'bus', 'with', 'ram']\n",
      "    Oración 2 (24 lemas): ['do', 'anyone', 'have', 'on', 'diamond', 'stealth', 'pro', 'local', 'bus', 'orchid', 'farenheit', '1280', 'ati', 'graphic', 'ultra', 'pro', 'any', 'other', 'vlb', 'card', 'please', 'post', 'or', 'email']\n",
      "    Oración 3 (2 lemas): ['thank', 'you']\n",
      "    Oración 4 (1 lemas): ['matt']\n",
      "\n",
      "--- Documento 3 ---\n",
      "  Original (1678 chars):\n",
      "'\n",
      "\n",
      "\n",
      "\n",
      "\tFinally you said what you dream about. Mediterranean???? That was new....\n",
      "\tThe area will be \"greater\" after some years, like your \"holocaust\" numbers......\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t*****\n",
      "\tIs't July in USA now????? Here in Sweden it's April and still cold.\n",
      "\tOr have you changed your calendar???\n",
      "\n",
      "\n",
      "\t\t\t\t\t\t    ****************\n",
      "\t\t\t\t\t\t    ******************\n",
      "\t\t\t    ***************\n",
      "\n",
      "\n",
      "\tNOTHING OF THE MENTIONED IS TRUE, BUT LET SAY IT's TRUE.\n",
      "\t\n",
      "\tSHALL THE AZERI WOMEN AND CHILDREN GOING TO PAY THE PRICE WITH\n",
      "\t\t\t\t\t\t    **************\n",
      "\tBEING RAPED, KILLED AND TORTURED BY THE ARMENIANS??????????\n",
      "\t\n",
      "\tHAVE YOU HEARDED SOMETHING CALLED: \"GENEVA CONVENTION\"???????\n",
      "\tYOU FACIST!!!!!\n",
      "\n",
      "\n",
      "\n",
      "\tOhhh i forgot, this is how Armenians fight, nobody has forgot\n",
      "\tyou killings, rapings and torture against the Kurds and Turks once\n",
      "\tupon a time!\n",
      "      \n",
      "       \n",
      "\n",
      "\n",
      "Ohhhh so swedish RedCross workers do lie they too? What ever you say\n",
      "\"regional killer\", if you don't like the person then shoot him that's your policy.....l\n",
      "\n",
      "\n",
      "\t\t\t\t\t\t\t\t\t\ti\n",
      "\t\t\t\t\t\t\t\t\t\ti\n",
      "\t\t\t\t\t\t\t\t\t\ti\n",
      "\tConfused?????\t\t\t\t\t\t\t\ti\n",
      "\t\t\t\t\t\t\t\t\t\ti\n",
      "        Search Turkish planes? You don't know what you are talking about.\ti\n",
      "        Turkey's government has announced that it's giving weapons  <-----------i\n",
      "        to Azerbadjan since Armenia started to attack Azerbadjan\t\t\n",
      "        it self, not the Karabag province. So why search a plane for weapons\t\n",
      "        since it's content is announced to be weapons?   \n",
      "\n",
      "\tIf there is one that's confused then that's you! We have the right (and we do)\n",
      "\tto give weapons to the Azeris, since Armenians started the fight in Azerbadjan!\n",
      " \n",
      "\n",
      "\n",
      "\tShoot down with what? Armenian bread and butter? Or the arms and personel \n",
      "\tof the Russian army?\n",
      "\n",
      "\n",
      "'\n",
      "\n",
      "  Número de oraciones detectadas: 21\n",
      "\n",
      "  Primeras oraciones procesadas (lemas):\n",
      "    Oración 1 (7 lemas): ['finally', 'you', 'say', 'what', 'you', 'dream', 'about']\n",
      "    Oración 2 (1 lemas): ['mediterranean']\n",
      "    Oración 3 (19 lemas): ['that', 'be', 'new', 'the', 'area', 'will', 'be', 'great', 'after', 'some', 'year', 'like', 'your', 'holocaust', 'number', 'july', 'in', 'usa', 'now']\n",
      "    Oración 4 (8 lemas): ['here', 'in', 'sweden', 'it', 'april', 'and', 'still', 'cold']\n",
      "    Oración 5 (6 lemas): ['or', 'have', 'you', 'change', 'your', 'calendar']\n",
      "    Oración 6 (11 lemas): ['nothing', 'of', 'the', 'mention', 'be', 'true', 'but', 'let', 'say', 'it', 'true']\n",
      "    Oración 7 (20 lemas): ['shall', 'the', 'azeri', 'woman', 'and', 'child', 'go', 'to', 'pay', 'the', 'price', 'with', 'be', 'rap', 'kill', 'and', 'torture', 'by', 'the', 'armenian']\n",
      "    Oración 8 (7 lemas): ['have', 'you', 'hearded', 'something', 'call', 'geneva', 'convention']\n",
      "    Oración 9 (2 lemas): ['you', 'facist']\n",
      "    Oración 10 (25 lemas): ['ohhh', 'i', 'forget', 'this', 'be', 'how', 'armenians', 'fight', 'nobody', 'have', 'forget', 'you', 'killing', 'rapings', 'and', 'torture', 'against', 'the', 'kurd', 'and', 'turk', 'once', 'upon', 'a', 'time']\n",
      "    Oración 11 (9 lemas): ['ohhhh', 'so', 'swedish', 'redcross', 'worker', 'do', 'lie', 'they', 'too']\n",
      "    Oración 12 (23 lemas): ['what', 'ever', 'you', 'say', 'regional', 'killer', 'if', 'you', 'do', 'like', 'the', 'person', 'then', 'shoot', 'him', 'that', 'your', 'policy', 'l', 'i', 'i', 'i', 'confuse']\n",
      "    Oración 13 (5 lemas): ['i', 'i', 'search', 'turkish', 'plane']\n",
      "    Oración 14 (8 lemas): ['you', 'do', 'know', 'what', 'you', 'be', 'talk', 'about']\n",
      "    Oración 15 (23 lemas): ['i', 'turkey', 'government', 'have', 'announce', 'that', 'it', 'give', 'weapon', 'to', 'azerbadjan', 'since', 'armenia', 'start', 'to', 'attack', 'azerbadjan', 'it', 'self', 'not', 'the', 'karabag', 'province']\n",
      "    Oración 16 (15 lemas): ['so', 'why', 'search', 'a', 'plane', 'for', 'weapon', 'since', 'it', 'content', 'be', 'announce', 'to', 'be', 'weapon']\n",
      "    Oración 17 (9 lemas): ['if', 'there', 'be', 'one', 'that', 'confuse', 'then', 'that', 'you']\n",
      "    Oración 18 (20 lemas): ['we', 'have', 'the', 'right', 'and', 'we', 'do', 'to', 'give', 'weapon', 'to', 'the', 'azeri', 'since', 'armenian', 'start', 'the', 'fight', 'in', 'azerbadjan']\n",
      "    Oración 19 (4 lemas): ['shoot', 'down', 'with', 'what']\n",
      "    Oración 20 (4 lemas): ['armenian', 'bread', 'and', 'butter']\n",
      "    Oración 21 (9 lemas): ['or', 'the', 'arm', 'and', 'personel', 'of', 'the', 'russian', 'army']\n",
      "\n",
      "--- Documento 4 ---\n",
      "  Original (781 chars):\n",
      "'\n",
      "Think!\n",
      "\n",
      "It's the SCSI card doing the DMA transfers NOT the disks...\n",
      "\n",
      "The SCSI card can do DMA transfers containing data from any of the SCSI devices\n",
      "it is attached when it wants to.\n",
      "\n",
      "An important feature of SCSI is the ability to detach a device. This frees the\n",
      "SCSI bus for other devices. This is typically used in a multi-tasking OS to\n",
      "start transfers on several devices. While each device is seeking the data the\n",
      "bus is free for other commands and data transfers. When the devices are\n",
      "ready to transfer the data they can aquire the bus and send the data.\n",
      "\n",
      "On an IDE bus when you start a transfer the bus is busy until the disk has seeked\n",
      "the data and transfered it. This is typically a 10-20ms second lock out for other\n",
      "processes wanting the bus irrespective of transfer time.\n",
      "'\n",
      "\n",
      "  Número de oraciones detectadas: 9\n",
      "\n",
      "  Primeras oraciones procesadas (lemas):\n",
      "    Oración 1 (1 lemas): ['think']\n",
      "    Oración 2 (33 lemas): ['it', 'the', 'scsi', 'card', 'do', 'the', 'dma', 'transfer', 'not', 'the', 'disk', 'the', 'scsi', 'card', 'can', 'do', 'dma', 'transfer', 'contain', 'data', 'from', 'any', 'of', 'the', 'scsi', 'device', 'it', 'be', 'attach', 'when', 'it', 'want', 'to']\n",
      "    Oración 3 (12 lemas): ['an', 'important', 'feature', 'of', 'scsi', 'be', 'the', 'ability', 'to', 'detach', 'a', 'device']\n",
      "    Oración 4 (8 lemas): ['this', 'free', 'the', 'scsi', 'bus', 'for', 'other', 'device']\n",
      "    Oración 5 (13 lemas): ['this', 'be', 'typically', 'use', 'in', 'a', 'o', 'to', 'start', 'transfer', 'on', 'several', 'device']\n",
      "    Oración 6 (17 lemas): ['while', 'each', 'device', 'be', 'seek', 'the', 'data', 'the', 'bus', 'be', 'free', 'for', 'other', 'command', 'and', 'data', 'transfer']\n",
      "    Oración 7 (18 lemas): ['when', 'the', 'device', 'be', 'ready', 'to', 'transfer', 'the', 'data', 'they', 'can', 'aquire', 'the', 'bus', 'and', 'send', 'the', 'data']\n",
      "    Oración 8 (23 lemas): ['on', 'an', 'ide', 'bus', 'when', 'you', 'start', 'a', 'transfer', 'the', 'bus', 'be', 'busy', 'until', 'the', 'disk', 'have', 'seek', 'the', 'data', 'and', 'transfer', 'it']\n",
      "    Oración 9 (17 lemas): ['this', 'be', 'typically', 'a', 'second', 'lock', 'out', 'for', 'other', 'process', 'want', 'the', 'bus', 'irrespective', 'of', 'transfer', 'time']\n",
      "\n",
      "--- Documento 5 ---\n",
      "  Original (666 chars):\n",
      "'1)    I have an old Jasmine drive which I cannot use with my new system.\n",
      " My understanding is that I have to upsate the driver with a more modern\n",
      "one in order to gain compatability with system 7.0.1.  does anyone know\n",
      "of an inexpensive program to do this?  ( I have seen formatters for <$20\n",
      "buit have no idea if they will work)\n",
      " \n",
      "2)     I have another ancient device, this one a tape drive for which\n",
      "the back utility freezes the system if I try to use it.  THe drive is a\n",
      "jasmine direct tape (bought used for $150 w/ 6 tapes, techmar\n",
      "mechanism).  Essentially I have the same question as above, anyone know\n",
      "of an inexpensive beckup utility I can use with system 7.0.1'\n",
      "\n",
      "  Número de oraciones detectadas: 5\n",
      "\n",
      "  Primeras oraciones procesadas (lemas):\n",
      "    Oración 1 (16 lemas): ['1', 'i', 'have', 'an', 'old', 'jasmine', 'drive', 'which', 'i', 'can', 'not', 'use', 'with', 'my', 'new', 'system']\n",
      "    Oración 2 (32 lemas): ['my', 'understanding', 'be', 'that', 'i', 'have', 'to', 'upsate', 'the', 'driver', 'with', 'a', 'more', 'modern', 'one', 'in', 'order', 'to', 'gain', 'compatability', 'with', 'system', 'do', 'anyone', 'know', 'of', 'an', 'inexpensive', 'program', 'to', 'do', 'this']\n",
      "    Oración 3 (39 lemas): ['i', 'have', 'see', 'formatters', 'for', '20', 'buit', 'have', 'no', 'idea', 'if', 'they', 'will', 'work', '2', 'i', 'have', 'another', 'ancient', 'device', 'this', 'one', 'a', 'tape', 'drive', 'for', 'which', 'the', 'back', 'utility', 'freeze', 'the', 'system', 'if', 'i', 'try', 'to', 'use', 'it']\n",
      "    Oración 4 (15 lemas): ['the', 'drive', 'be', 'a', 'jasmine', 'direct', 'tape', 'buy', 'use', 'for', '150', '6', 'tape', 'techmar', 'mechanism']\n",
      "    Oración 5 (20 lemas): ['essentially', 'i', 'have', 'the', 'same', 'question', 'a', 'above', 'anyone', 'know', 'of', 'an', 'inexpensive', 'beckup', 'utility', 'i', 'can', 'use', 'with', 'system']\n",
      "\n",
      "--- Documento 6 ---\n",
      "  Original (387 chars):\n",
      "'\n",
      "\n",
      "Back in high school I worked as a lab assistant for a bunch of experimental\n",
      "psychologists at Bell Labs.  When they were doing visual perception and\n",
      "memory experiments, they used vector-type displays, with 1-millisecond\n",
      "refresh rates common.\n",
      "\n",
      "So your case of 1/200th sec is quite practical, and the experimenters were\n",
      "probably sure that it was 5 milliseconds, not 4 or 6 either.\n",
      "\n",
      "\n",
      "Steve'\n",
      "\n",
      "  Número de oraciones detectadas: 4\n",
      "\n",
      "  Primeras oraciones procesadas (lemas):\n",
      "    Oración 1 (19 lemas): ['back', 'in', 'high', 'school', 'i', 'work', 'a', 'a', 'lab', 'assistant', 'for', 'a', 'bunch', 'of', 'experimental', 'psychologist', 'at', 'bell', 'lab']\n",
      "    Oración 2 (16 lemas): ['when', 'they', 'be', 'do', 'visual', 'perception', 'and', 'memory', 'experiment', 'they', 'use', 'display', 'with', 'refresh', 'rate', 'common']\n",
      "    Oración 3 (24 lemas): ['so', 'your', 'case', 'of', 'sec', 'be', 'quite', 'practical', 'and', 'the', 'experimenter', 'be', 'probably', 'sure', 'that', 'it', 'be', '5', 'millisecond', 'not', '4', 'or', '6', 'either']\n",
      "    Oración 4 (1 lemas): ['steve']\n"
     ]
    }
   ],
   "source": [
    "num_examples_to_show = 6\n",
    "print(f\"\\nMostrando los primeros {num_examples_to_show} ejemplos procesados:\")\n",
    "\n",
    "for i in range(min(num_examples_to_show, len(first_batch))):\n",
    "    print(f\"\\n--- Documento {i+1} ---\")\n",
    "    original_text = first_batch[i]\n",
    "    processed_doc = processed_batch_segmented[i]\n",
    "\n",
    "    print(f\"  Original ({len(original_text)} chars):\\n'{original_text}'\")\n",
    "    print(f\"\\n  Número de oraciones detectadas: {len(processed_doc)}\")\n",
    "    print(\"\\n  Primeras oraciones procesadas (lemas):\")\n",
    "    for j, sentence_lemmas in enumerate(processed_doc):\n",
    "        print(f\"    Oración {j+1} ({len(sentence_lemmas)} lemas): {sentence_lemmas}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
