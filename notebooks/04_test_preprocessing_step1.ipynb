{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento - Normalización Unicode y Minúsculas\n",
    "\n",
    "**Objetivo:** Implementar los primeros pasos de preprocesamiento sobre un lote de documentos:\n",
    "1. Normalización Unicode (NFKC).\n",
    "2. Conversión a minúsculas.\n",
    "Debemos finalmente verificar los resultados comparando el texto original con el procesado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import unicodedata\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos la funcion generadora para dividir la lista de datos en lotes que vimos\n",
    "def batch_generator(data_list, batch_size):\n",
    "    \"\"\"\n",
    "    Generador que divide una lista de datos en lotes (batches).\n",
    "\n",
    "    Args:\n",
    "        data_list (list): La lista completa de datos a dividir.\n",
    "        batch_size (int): El tamaño deseado para cada lote.\n",
    "\n",
    "    Yields:\n",
    "        list: Un lote  de datos. El último lote puede ser más pequeño.\n",
    "              No produce nada si la lista de entrada está vacía o batch_size <= 0.\n",
    "    \"\"\"\n",
    "    if not data_list or batch_size <= 0:\n",
    "        print(\"Advertencia: La lista de datos está vacía o el tamaño de lote es inválido. No se generarán lotes.\")\n",
    "        return\n",
    "\n",
    "    num_total_items = len(data_list)\n",
    "    # Iteramos sobre la lista en pasos de tamaño batch_size\n",
    "    for start_index in range(0, num_total_items, batch_size):\n",
    "        end_index = start_index + batch_size\n",
    "        batch = data_list[start_index:end_index]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado en 1.12 segundos.\n",
      "Número total de documentos: 18846\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "newsgroups_data = fetch_20newsgroups(subset='all',\n",
    "                                     remove=('headers', 'footers', 'quotes'))\n",
    "end_time = time.time()\n",
    "print(f\"Dataset cargado en {end_time - start_time:.2f} segundos.\")\n",
    "\n",
    "documents = newsgroups_data.data\n",
    "num_docs = len(documents)\n",
    "print(f\"Número total de documentos: {num_docs}\")\n",
    "\n",
    "del newsgroups_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Obteniendo el primer lote (batch_size = 32)...\n",
      "Primer lote obtenido con 32 documentos.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # Usamos un tamaño de lote más pequeño para la prueba\n",
    "print(f\"\\nObteniendo el primer lote (batch_size = {batch_size})...\")\n",
    "\n",
    "# Crear el generador y obtener el primer lote\n",
    "batch_gen = batch_generator(documents, batch_size)\n",
    "first_batch = next(batch_gen, None) # next() obtiene el primer elemento del generador\n",
    "\n",
    "if first_batch:\n",
    "    print(f\"Primer lote obtenido con {len(first_batch)} documentos.\")\n",
    "else:\n",
    "    print(\"No se pudo obtener el primer lote.\")\n",
    "    # Podríamos detenernos aquí si no hay datos\n",
    "    first_batch = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Que es Unicode?\n",
    "\"El estándar de codificación de caracteres Unicode es un esquema de codificación de caracteres de longitud fija que incluye caracteres de casi todas las lenguas vivas del mundo\" (IBM).\n",
    "\n",
    "## Normalizacion de series Unicode\n",
    "\n",
    "\" Las series Unicode pueden ser equivalentes canónicamente o equivalentes desde el punto de vista de la compatibilidad. Si son equivalentes canónicamente, también son equivalentes desde el punto de vista de la compatibilidad. \" (IBM)\n",
    "\n",
    "Debemos tener en cuenta que \"Canónicamente equivalente\" significa que son representaciones diferentes pero lucen y funcionan exactamente igual (como 'é' y 'e'+'´'). \"Compatibilidad equivalente\" es más amplio, incluye caracteres que representan la misma idea pero pueden verse diferente o tener un formato especial (como una ligadura 'ﬁ' vs. 'f'+'i', o un número en superíndice ¹ vs. el número normal '1')\n",
    "\n",
    "Existen varias formas de normalizacion:\n",
    "\n",
    "* NFD (descomposición canónica de formato de normalización): Los caracteres se descomponen según su equivalencia canónica.\n",
    "* NFC (composición canónica de formato de normalización): Los caracteres se descomponen y después se recomponen según su equivalencia canónica.\n",
    "* NFKD (descomposición de compatibilidad de formato de normalización): Los caracteres se descomponen según su equivalencia de compatibilidad.\n",
    "* Composición de compatibilidad de formato de normalización (NFKC): Los caracteres se descomponen según su equivalencia de compatibilidad y después se recomponen según su equivalencia canónica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_step1(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        normalized_text = unicodedata.normalize('NFKC', text) # normalizamos el texto\n",
    "        lower_text = normalized_text.lower() # convertimos a minusculas\n",
    "\n",
    "        return lower_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mostrando los primeros 3 ejemplos del lote:\n",
      "\n",
      "--- Ejemplo 1 ---\n",
      "  Original (712 chars):\n",
      "'\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' re...'\n",
      "\n",
      "  Procesado (712 chars):\n",
      "'\n",
      "\n",
      "i am sure some bashers of pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent pens massacre of the devils. actually,\n",
      "i am  bit puzzled too and a bit relieved. however, i am going to put an end\n",
      "to non-pittsburghers' re...'\n",
      "\n",
      "--- Ejemplo 2 ---\n",
      "  Original (324 chars):\n",
      "'My brother is in the market for a high-performance video card that supports\n",
      "VESA local bus with 1-2MB RAM.  Does anyone have suggestions/ideas on:\n",
      "\n",
      "  - Diamond Stealth Pro Local Bus\n",
      "\n",
      "  - Orchid Farenheit 1280\n",
      "\n",
      "  - ATI Graphics Ultra Pro\n",
      "\n",
      "  - Any othe...'\n",
      "\n",
      "  Procesado (324 chars):\n",
      "'my brother is in the market for a high-performance video card that supports\n",
      "vesa local bus with 1-2mb ram.  does anyone have suggestions/ideas on:\n",
      "\n",
      "  - diamond stealth pro local bus\n",
      "\n",
      "  - orchid farenheit 1280\n",
      "\n",
      "  - ati graphics ultra pro\n",
      "\n",
      "  - any othe...'\n",
      "\n",
      "--- Ejemplo 3 ---\n",
      "  Original (1678 chars):\n",
      "'\n",
      "\n",
      "\n",
      "\n",
      "\tFinally you said what you dream about. Mediterranean???? That was new....\n",
      "\tThe area will be \"greater\" after some years, like your \"holocaust\" numbers......\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t*****\n",
      "\tIs't July in USA now????? Here in Sweden it's April and still cold.\n",
      "\tOr have...'\n",
      "\n",
      "  Procesado (1678 chars):\n",
      "'\n",
      "\n",
      "\n",
      "\n",
      "\tfinally you said what you dream about. mediterranean???? that was new....\n",
      "\tthe area will be \"greater\" after some years, like your \"holocaust\" numbers......\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t*****\n",
      "\tis't july in usa now????? here in sweden it's april and still cold.\n",
      "\tor have...'\n"
     ]
    }
   ],
   "source": [
    "# Normalizamos el primer lote\n",
    "processed_batch = [preprocess_text_step1(doc) for doc in first_batch]\n",
    "num_examples_to_show = 3\n",
    "print(f\"\\nMostrando los primeros {num_examples_to_show} ejemplos del lote:\")\n",
    "\n",
    "for i in range(min(num_examples_to_show, len(first_batch))):\n",
    "    print(f\"\\n--- Ejemplo {i+1} ---\")\n",
    "    print(f\"  Original ({len(first_batch[i])} chars):\\n'{first_batch[i][:250]}...'\")\n",
    "    print(f\"\\n  Procesado ({len(processed_batch[i])} chars):\\n'{processed_batch[i][:250]}...'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
