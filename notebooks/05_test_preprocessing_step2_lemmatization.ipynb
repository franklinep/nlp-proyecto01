{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Preprocesamiento: Lematización con NLTK\n",
    "\n",
    "**Objetivo:** Integrar la lematización usando NLTK al pipeline de preprocesamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    print(f\"Añadido '{module_path}' a sys.path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos las funciones necesarias usadas en el notebook anterior\n",
    "from src.functions import batch_generator, preprocess_text_step1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapeo de etiquetas POS\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Convierte etiquetas POS de Penn Treebank a formato WordNet.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging\n",
    "Es una tecnica comun utilizada para realizar la clasificacion de palabras en una texto (corpus), basado en las definiciones de las palabras y su contexto.\n",
    "\n",
    "Estamos usando un etiquetador `nltk.pos_tag` que usa un conjunto de etiquetas `Penn Treebank Tagset` el cual este es muy detallado, por ejemplo :\n",
    "\n",
    "- NN : Sustantivo singular\n",
    "- NNS : Sustantivo plural\n",
    "- VBG : Verbo gerundio\n",
    "- VBD : Verbo pasado, etc ...\n",
    "\n",
    "El lematizador `WordNetLemmatizer()` de NLTK espera un conjunto de etiquetas muy simple, basado solo en categorias principales:\n",
    " \n",
    "- Sustantivo\n",
    "- Verbo\n",
    "- Adjetivo\n",
    "- Adverbio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos el lematizador de WordNet\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de documentos: 18846\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el dataset de 20 Newsgroups\n",
    "newsgroups_data = fetch_20newsgroups(subset='all',\n",
    "                                     remove=('headers', 'footers', 'quotes'),\n",
    "                                     data_home='./data/20newsgroups_cache')\n",
    "documents = newsgroups_data.data\n",
    "num_docs = len(documents)\n",
    "print(f\"Número total de documentos: {num_docs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_lemmatize_nltk(text_chunk):\n",
    "    \"\"\"\n",
    "    Pipeline completo: Normaliza, tokeniza, etiqueta POS y lematiza con NLTK.\n",
    "    \"\"\"\n",
    "    # normaliza y convierte a minúsculas\n",
    "    normalized_lower_chunk = preprocess_text_step1(text_chunk)\n",
    "    # tokeniza usando ingles\n",
    "    tokens = nltk.word_tokenize(normalized_lower_chunk, language='english')\n",
    "    # realizamos el etiqueado pos\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    # lematizamos\n",
    "    lemmas = []\n",
    "    for word, tag in pos_tags:\n",
    "        # filtra tokens no alfanuméricos\n",
    "        if word.isalnum():\n",
    "            wordnet_tag = get_wordnet_pos(tag)\n",
    "            lemma = lemmatizer.lemmatize(word, pos=wordnet_tag)\n",
    "            lemmas.append(lemma)\n",
    "\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NO son alfanuméricos:*\n",
    "- Espacios en blanco (' ')\n",
    "- Signos de puntuación (., ,, ;, !, ?, -, etc.)\n",
    "- Símbolos ($, %, &, #, @, etc.)\n",
    "- Caracteres de control (como saltos de línea \\n o tabulaciones \\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba del Pipeline en un lote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenemos un lote de prueba\n",
    "batch_size = 10\n",
    "batch_gen = batch_generator(documents, batch_size)\n",
    "first_batch = next(batch_gen, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de procesamiento para el primer lote: 0.0173 segundos\n"
     ]
    }
   ],
   "source": [
    "start_proc_time = time.time()\n",
    "#  processed_batch contiene [doc1, doc2, ...] donde cada doc es una lista de tokens lematizados [\"lemma1\", \"lemma2\", ...]\n",
    "processed_lemmas_batch = [preprocess_and_lemmatize_nltk(doc) for doc in first_batch]\n",
    "end_proc_time = time.time()\n",
    "print(f\"Tiempo de procesamiento para el primer lote: {end_proc_time - start_proc_time:.4f} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mostrando los primeros 3 ejemplos procesados del lote:\n",
      "\n",
      "--- Ejemplo 1 ---\n",
      "  Original (712 chars):\n",
      "'\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "'\n",
      "\n",
      "  Normalizado/Minúsculas (712 chars):\n",
      "'\n",
      "\n",
      "i am sure some bashers of pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent pens massacre of the devils. actually,\n",
      "i am  bit puzzled too and a bit relieved. however, i am going to put an end\n",
      "to non-pittsburghers' relief with a bit of praise for the pens. man, they\n",
      "are killing those devils worse than i thought. jagr just showed you why\n",
      "he is much better than his regular season stats. he is also a lot\n",
      "fo fun to watch in the playoffs. bowman should let jagr have a lot of\n",
      "fun in the next couple of games since the pens are going to beat the pulp out of jersey anyway. i was very disappointed not to see the islanders lose the final\n",
      "regular season game.          pens rule!!!\n",
      "\n",
      "'\n",
      "\n",
      "  Lemas (136 tokens):\n",
      "['i', 'be', 'sure', 'some', 'bashers', 'of', 'pen', 'fan', 'be', 'pretty', 'confuse', 'about', 'the', 'lack', 'of', 'any', 'kind', 'of', 'post', 'about', 'the', 'recent', 'pen', 'massacre', 'of', 'the', 'devil', 'actually', 'i', 'be', 'bit', 'puzzled', 'too', 'and', 'a', 'bit', 'relieved', 'however', 'i', 'be', 'go', 'to', 'put', 'an', 'end', 'to', 'relief', 'with', 'a', 'bit', 'of', 'praise', 'for', 'the', 'pen', 'man', 'they', 'be', 'kill', 'those', 'devil', 'bad', 'than', 'i', 'thought', 'jagr', 'just', 'show', 'you', 'why', 'he', 'be', 'much', 'good', 'than', 'his', 'regular', 'season', 'stats', 'he', 'be', 'also', 'a', 'lot', 'fo', 'fun', 'to', 'watch', 'in', 'the', 'playoff', 'bowman', 'should', 'let', 'jagr', 'have', 'a', 'lot', 'of', 'fun', 'in', 'the', 'next', 'couple', 'of', 'game', 'since', 'the', 'pen', 'be', 'go', 'to', 'beat', 'the', 'pulp', 'out', 'of', 'jersey', 'anyway', 'i', 'be', 'very', 'disappointed', 'not', 'to', 'see', 'the', 'islander', 'lose', 'the', 'final', 'regular', 'season', 'game', 'pen', 'rule']\n",
      "\n",
      "--- Ejemplo 2 ---\n",
      "  Original (324 chars):\n",
      "'My brother is in the market for a high-performance video card that supports\n",
      "VESA local bus with 1-2MB RAM.  Does anyone have suggestions/ideas on:\n",
      "\n",
      "  - Diamond Stealth Pro Local Bus\n",
      "\n",
      "  - Orchid Farenheit 1280\n",
      "\n",
      "  - ATI Graphics Ultra Pro\n",
      "\n",
      "  - Any other high-performance VLB card\n",
      "\n",
      "\n",
      "Please post or email.  Thank you!\n",
      "\n",
      "  - Matt\n",
      "'\n",
      "\n",
      "  Normalizado/Minúsculas (324 chars):\n",
      "'my brother is in the market for a high-performance video card that supports\n",
      "vesa local bus with 1-2mb ram.  does anyone have suggestions/ideas on:\n",
      "\n",
      "  - diamond stealth pro local bus\n",
      "\n",
      "  - orchid farenheit 1280\n",
      "\n",
      "  - ati graphics ultra pro\n",
      "\n",
      "  - any other high-performance vlb card\n",
      "\n",
      "\n",
      "please post or email.  thank you!\n",
      "\n",
      "  - matt\n",
      "'\n",
      "\n",
      "  Lemas (44 tokens):\n",
      "['my', 'brother', 'be', 'in', 'the', 'market', 'for', 'a', 'video', 'card', 'that', 'support', 'vesa', 'local', 'bus', 'with', 'ram', 'do', 'anyone', 'have', 'on', 'diamond', 'stealth', 'pro', 'local', 'bus', 'orchid', 'farenheit', '1280', 'ati', 'graphic', 'ultra', 'pro', 'any', 'other', 'vlb', 'card', 'please', 'post', 'or', 'email', 'thank', 'you', 'matt']\n",
      "\n",
      "--- Ejemplo 3 ---\n",
      "  Original (1678 chars):\n",
      "'\n",
      "\n",
      "\n",
      "\n",
      "\tFinally you said what you dream about. Mediterranean???? That was new....\n",
      "\tThe area will be \"greater\" after some years, like your \"holocaust\" numbers......\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t*****\n",
      "\tIs't July in USA now????? Here in Sweden it's April and still cold.\n",
      "\tOr have you changed your calendar???\n",
      "\n",
      "\n",
      "\t\t\t\t\t\t    ****************\n",
      "\t\t\t\t\t\t    ******************\n",
      "\t\t\t    ***************\n",
      "\n",
      "\n",
      "\tNOTHING OF THE MENTIONED IS TRUE, BUT LET SAY IT's TRUE.\n",
      "\t\n",
      "\tSHALL THE AZERI WOMEN AND CHILDREN GOING TO PAY THE PRICE WITH\n",
      "\t\t\t\t\t\t    **************\n",
      "\tBEING RAPED, KILLED AND TORTURED BY THE ARMENIANS??????????\n",
      "\t\n",
      "\tHAVE YOU HEARDED SOMETHING CALLED: \"GENEVA CONVENTION\"???????\n",
      "\tYOU FACIST!!!!!\n",
      "\n",
      "\n",
      "\n",
      "\tOhhh i forgot, this is how Armenians fight, nobody has forgot\n",
      "\tyou killings, rapings and torture against the Kurds and Turks once\n",
      "\tupon a time!\n",
      "      \n",
      "       \n",
      "\n",
      "\n",
      "Ohhhh so swedish RedCross workers do lie they too? What ever you say\n",
      "\"regional killer\", if you don't like the person then shoot him that's your policy.....l\n",
      "\n",
      "\n",
      "\t\t\t\t\t\t\t\t\t\ti\n",
      "\t\t\t\t\t\t\t\t\t\ti\n",
      "\t\t\t\t\t\t\t\t\t\ti\n",
      "\tConfused?????\t\t\t\t\t\t\t\ti\n",
      "\t\t\t\t\t\t\t\t\t\ti\n",
      "        Search Turkish planes? You don't know what you are talking about.\ti\n",
      "        Turkey's government has announced that it's giving weapons  <-----------i\n",
      "        to Azerbadjan since Armenia started to attack Azerbadjan\t\t\n",
      "        it self, not the Karabag province. So why search a plane for weapons\t\n",
      "        since it's content is announced to be weapons?   \n",
      "\n",
      "\tIf there is one that's confused then that's you! We have the right (and we do)\n",
      "\tto give weapons to the Azeris, since Armenians started the fight in Azerbadjan!\n",
      " \n",
      "\n",
      "\n",
      "\tShoot down with what? Armenian bread and butter? Or the arms and personel \n",
      "\tof the Russian army?\n",
      "\n",
      "\n",
      "'\n",
      "\n",
      "  Normalizado/Minúsculas (1678 chars):\n",
      "'\n",
      "\n",
      "\n",
      "\n",
      "\tfinally you said what you dream about. mediterranean???? that was new....\n",
      "\tthe area will be \"greater\" after some years, like your \"holocaust\" numbers......\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t*****\n",
      "\tis't july in usa now????? here in sweden it's april and still cold.\n",
      "\tor have you changed your calendar???\n",
      "\n",
      "\n",
      "\t\t\t\t\t\t    ****************\n",
      "\t\t\t\t\t\t    ******************\n",
      "\t\t\t    ***************\n",
      "\n",
      "\n",
      "\tnothing of the mentioned is true, but let say it's true.\n",
      "\t\n",
      "\tshall the azeri women and children going to pay the price with\n",
      "\t\t\t\t\t\t    **************\n",
      "\tbeing raped, killed and tortured by the armenians??????????\n",
      "\t\n",
      "\thave you hearded something called: \"geneva convention\"???????\n",
      "\tyou facist!!!!!\n",
      "\n",
      "\n",
      "\n",
      "\tohhh i forgot, this is how armenians fight, nobody has forgot\n",
      "\tyou killings, rapings and torture against the kurds and turks once\n",
      "\tupon a time!\n",
      "      \n",
      "       \n",
      "\n",
      "\n",
      "ohhhh so swedish redcross workers do lie they too? what ever you say\n",
      "\"regional killer\", if you don't like the person then shoot him that's your policy.....l\n",
      "\n",
      "\n",
      "\t\t\t\t\t\t\t\t\t\ti\n",
      "\t\t\t\t\t\t\t\t\t\ti\n",
      "\t\t\t\t\t\t\t\t\t\ti\n",
      "\tconfused?????\t\t\t\t\t\t\t\ti\n",
      "\t\t\t\t\t\t\t\t\t\ti\n",
      "        search turkish planes? you don't know what you are talking about.\ti\n",
      "        turkey's government has announced that it's giving weapons  <-----------i\n",
      "        to azerbadjan since armenia started to attack azerbadjan\t\t\n",
      "        it self, not the karabag province. so why search a plane for weapons\t\n",
      "        since it's content is announced to be weapons?   \n",
      "\n",
      "\tif there is one that's confused then that's you! we have the right (and we do)\n",
      "\tto give weapons to the azeris, since armenians started the fight in azerbadjan!\n",
      " \n",
      "\n",
      "\n",
      "\tshoot down with what? armenian bread and butter? or the arms and personel \n",
      "\tof the russian army?\n",
      "\n",
      "\n",
      "'\n",
      "\n",
      "  Lemas (235 tokens):\n",
      "['finally', 'you', 'say', 'what', 'you', 'dream', 'about', 'mediterranean', 'that', 'be', 'new', 'the', 'area', 'will', 'be', 'great', 'after', 'some', 'year', 'like', 'your', 'holocaust', 'number', 'july', 'in', 'usa', 'now', 'here', 'in', 'sweden', 'it', 'april', 'and', 'still', 'cold', 'or', 'have', 'you', 'change', 'your', 'calendar', 'nothing', 'of', 'the', 'mention', 'be', 'true', 'but', 'let', 'say', 'it', 'true', 'shall', 'the', 'azeri', 'woman', 'and', 'child', 'go', 'to', 'pay', 'the', 'price', 'with', 'be', 'rap', 'kill', 'and', 'torture', 'by', 'the', 'armenian', 'have', 'you', 'hearded', 'something', 'call', 'geneva', 'convention', 'you', 'facist', 'ohhh', 'i', 'forget', 'this', 'be', 'how', 'armenians', 'fight', 'nobody', 'have', 'forget', 'you', 'killing', 'rapings', 'and', 'torture', 'against', 'the', 'kurd', 'and', 'turk', 'once', 'upon', 'a', 'time', 'ohhhh', 'so', 'swedish', 'redcross', 'worker', 'do', 'lie', 'they', 'too', 'what', 'ever', 'you', 'say', 'regional', 'killer', 'if', 'you', 'do', 'like', 'the', 'person', 'then', 'shoot', 'him', 'that', 'your', 'policy', 'l', 'i', 'i', 'i', 'confuse', 'i', 'i', 'search', 'turkish', 'plane', 'you', 'do', 'know', 'what', 'you', 'be', 'talk', 'about', 'i', 'turkey', 'government', 'have', 'announce', 'that', 'it', 'give', 'weapon', 'to', 'azerbadjan', 'since', 'armenia', 'start', 'to', 'attack', 'azerbadjan', 'it', 'self', 'not', 'the', 'karabag', 'province', 'so', 'why', 'search', 'a', 'plane', 'for', 'weapon', 'since', 'it', 'content', 'be', 'announce', 'to', 'be', 'weapon', 'if', 'there', 'be', 'one', 'that', 'confuse', 'then', 'that', 'you', 'we', 'have', 'the', 'right', 'and', 'we', 'do', 'to', 'give', 'weapon', 'to', 'the', 'azeri', 'since', 'armenian', 'start', 'the', 'fight', 'in', 'azerbadjan', 'shoot', 'down', 'with', 'what', 'armenian', 'bread', 'and', 'butter', 'or', 'the', 'arm', 'and', 'personel', 'of', 'the', 'russian', 'army']\n"
     ]
    }
   ],
   "source": [
    "num_examples_to_show = 3\n",
    "print(f\"\\nMostrando los primeros {num_examples_to_show} ejemplos procesados del lote:\")\n",
    "\n",
    "for i in range(min(num_examples_to_show, len(first_batch))):\n",
    "    print(f\"\\n--- Ejemplo {i+1} ---\")\n",
    "    original_text = first_batch[i]\n",
    "    processed_lemmas = processed_lemmas_batch[i]\n",
    "    #  texto original\n",
    "    print(f\"  Original ({len(original_text)} chars):\\n'{original_text}'\")\n",
    "    #  texto normalizado/minúsculas\n",
    "    normalize_text = preprocess_text_step1(original_text)\n",
    "    print(f\"\\n  Normalizado/Minúsculas ({len(normalize_text)} chars):\\n'{normalize_text}'\")\n",
    "    # los lemas resultantes\n",
    "    print(f\"\\n  Lemas ({len(processed_lemmas)} tokens):\\n{processed_lemmas}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
