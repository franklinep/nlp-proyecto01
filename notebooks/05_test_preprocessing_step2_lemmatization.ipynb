{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Preprocesamiento: Lematización con NLTK\n",
    "\n",
    "**Objetivo:** Integrar la lematización usando NLTK al pipeline de preprocesamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos las funciones necesarias usadas en el notebook anterior\n",
    "from src.functions import batch_generator, preprocess_text_step1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapeo de etiquetas POS\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Convierte etiquetas POS de Penn Treebank a formato WordNet.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging\n",
    "Es una tecnica comun utilizada para realizar la clasificacion de palabras en una texto (corpus), basado en las definiciones de las palabras y su contexto.\n",
    "\n",
    "Estamos usando un etiquetador `nltk.pos_tag` que usa un conjunto de etiquetas `Penn Treebank Tagset` el cual este es muy detallado, por ejemplo :\n",
    "\n",
    "- NN : Sustantivo singular\n",
    "- NNS : Sustantivo plural\n",
    "- VBG : Verbo gerundio\n",
    "- VBD : Verbo pasado, etc ...\n",
    "\n",
    "El lematizador `WordNetLemmatizer()` de NLTK espera un conjunto de etiquetas muy simple, basado solo en categorias principales:\n",
    " \n",
    "- Sustantivo\n",
    "- Verbo\n",
    "- Adjetivo\n",
    "- Adverbio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos el lematizador de WordNet\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de documentos: 18846\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el dataset de 20 Newsgroups\n",
    "newsgroups_data = fetch_20newsgroups(subset='all',\n",
    "                                     remove=('headers', 'footers', 'quotes'),\n",
    "                                     data_home='./data/20newsgroups_cache')\n",
    "documents = newsgroups_data.data\n",
    "num_docs = len(documents)\n",
    "print(f\"Número total de documentos: {num_docs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_lemmatize_nltk(text_chunk):\n",
    "    \"\"\"\n",
    "    Pipeline completo: Normaliza, tokeniza, etiqueta POS y lematiza con NLTK.\n",
    "    \"\"\"\n",
    "    # normaliza y convierte a minúsculas\n",
    "    normalized_lower_chunk = preprocess_text_step1(text_chunk)\n",
    "    # tokeniza usando ingles\n",
    "    tokens = nltk.word_tokenize(normalized_lower_chunk, language='english')\n",
    "    # realizamos el etiqueado pos\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    # lematizamos\n",
    "    lemmas = []\n",
    "    for word, tag in pos_tags:\n",
    "        # filtra tokens no alfanuméricos\n",
    "        if word.isalnum():\n",
    "            wordnet_tag = get_wordnet_pos(tag)\n",
    "            lemma = lemmatizer.lemmatize(word, pos=wordnet_tag)\n",
    "            lemmas.append(lemma)\n",
    "\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NO son alfanuméricos:*\n",
    "- Espacios en blanco (' ')\n",
    "- Signos de puntuación (., ,, ;, !, ?, -, etc.)\n",
    "- Símbolos ($, %, &, #, @, etc.)\n",
    "- Caracteres de control (como saltos de línea \\n o tabulaciones \\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba del Pipeline en un lote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenemos un lote de prueba\n",
    "batch_size = 10\n",
    "batch_gen = batch_generator(documents, batch_size)\n",
    "first_batch = next(batch_gen, [])\n",
    "processed_lemmas_batch = [preprocess_and_lemmatize_nltk(doc) for doc in first_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mostrando los primeros 3 ejemplos procesados del lote:\n",
      "\n",
      "--- Ejemplo 1 ---\n",
      "  Original (712 chars):\n",
      "'\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about ...'\n",
      "\n",
      "  Normalizado/Minúsculas (712 chars):\n",
      "'\n",
      "\n",
      "i am sure some bashers of pens fans are pretty confused about the lack\n",
      "of any kind of posts about ...'\n",
      "\n",
      "  Lemas (136 tokens):\n",
      "['i', 'be', 'sure', 'some', 'bashers', 'of', 'pen', 'fan', 'be', 'pretty', 'confuse', 'about', 'the', 'lack', 'of', 'any', 'kind', 'of', 'post', 'about', 'the', 'recent', 'pen', 'massacre', 'of', 'the', 'devil', 'actually', 'i', 'be']...\n",
      "\n",
      "--- Ejemplo 2 ---\n",
      "  Original (324 chars):\n",
      "'My brother is in the market for a high-performance video card that supports\n",
      "VESA local bus with 1-2M...'\n",
      "\n",
      "  Normalizado/Minúsculas (324 chars):\n",
      "'my brother is in the market for a high-performance video card that supports\n",
      "vesa local bus with 1-2m...'\n",
      "\n",
      "  Lemas (44 tokens):\n",
      "['my', 'brother', 'be', 'in', 'the', 'market', 'for', 'a', 'video', 'card', 'that', 'support', 'vesa', 'local', 'bus', 'with', 'ram', 'do', 'anyone', 'have', 'on', 'diamond', 'stealth', 'pro', 'local', 'bus', 'orchid', 'farenheit', '1280', 'ati']...\n",
      "\n",
      "--- Ejemplo 3 ---\n",
      "  Original (1678 chars):\n",
      "'\n",
      "\n",
      "\n",
      "\n",
      "\tFinally you said what you dream about. Mediterranean???? That was new....\n",
      "\tThe area will be \"gr...'\n",
      "\n",
      "  Normalizado/Minúsculas (1678 chars):\n",
      "'\n",
      "\n",
      "\n",
      "\n",
      "\tfinally you said what you dream about. mediterranean???? that was new....\n",
      "\tthe area will be \"gr...'\n",
      "\n",
      "  Lemas (235 tokens):\n",
      "['finally', 'you', 'say', 'what', 'you', 'dream', 'about', 'mediterranean', 'that', 'be', 'new', 'the', 'area', 'will', 'be', 'great', 'after', 'some', 'year', 'like', 'your', 'holocaust', 'number', 'july', 'in', 'usa', 'now', 'here', 'in', 'sweden']...\n"
     ]
    }
   ],
   "source": [
    "num_examples_to_show = 3\n",
    "print(f\"\\nMostrando los primeros {num_examples_to_show} ejemplos procesados del lote:\")\n",
    "\n",
    "for i in range(min(num_examples_to_show, len(first_batch))):\n",
    "    print(f\"\\n--- Ejemplo {i+1} ---\")\n",
    "    original_text = first_batch[i]\n",
    "    processed_lemmas = processed_lemmas_batch[i]\n",
    "    #  texto original\n",
    "    print(f\"  Original ({len(original_text)} chars):\\n'{original_text[:100]}...'\")\n",
    "    #  texto normalizado/minúsculas\n",
    "    normalize_text = preprocess_text_step1(original_text)\n",
    "    print(f\"\\n  Normalizado/Minúsculas ({len(normalize_text)} chars):\\n'{normalize_text[:100]}...'\")\n",
    "    # los lemas resultantes\n",
    "    print(f\"\\n  Lemas ({len(processed_lemmas)} tokens):\\n{processed_lemmas[:30]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
